{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo notebook, we'll create some hybrid data from the [single phase 3A data set](http://data.cortexlab.net/singlePhase3/) provided by Nick Steinmetz and the Cortex Lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import glob\n",
    "import hashlib\n",
    "import importlib\n",
    "import os\n",
    "import os.path as op\n",
    "import shutil\n",
    "import sys\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# add folder containing our library to sys.path\n",
    "factory_dir = op.abspath(\"../..\")\n",
    "if factory_dir not in sys.path:\n",
    "    sys.path.insert(0, factory_dir)\n",
    "\n",
    "import factory.generate.shift\n",
    "import factory.generate.generators\n",
    "import factory.io.phy\n",
    "import factory.io.raw\n",
    "import factory.io.gt\n",
    "from factory.io.logging import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_dir = op.abspath(op.join(\".\", \"cortex_demo\"))\n",
    "\n",
    "if not op.isdir(hybrid_dir):\n",
    "    os.mkdir(hybrid_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now set our parameters. The copy would be too long to wait, so we assume `raw_source_file` and `raw_target_file` already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter(object):\n",
    "    pass\n",
    "\n",
    "params = Parameter()\n",
    "\n",
    "# path to file containing raw source data\n",
    "params.raw_source_file = r\"F:/CortexLab/singlePhase3/data/Hopkins_20160722_g0_t0.imec.ap_CAR.bin\"\n",
    "# path to file to contain hybrid\n",
    "params.raw_target_file = op.join(hybrid_dir, \"Hopkins_20160722_g0_t0.imec.ap_CAR.GT.bin\")\n",
    "# type of raw data, as a numpy dtype\n",
    "params.data_type = np.int16\n",
    "# sample rate in Hz\n",
    "params.sample_rate = 30000\n",
    "# directory containing output from KiloSort\n",
    "params.data_directory = \"F:/CortexLab/singlePhase3/data\"\n",
    "# type of output from KiloSort, in this case phy input\n",
    "params.output_type = \"phy\"\n",
    "# probe layout\n",
    "params.probe_type = \"npix3a\"\n",
    "# indices (cluster labels) of ground-truth units\n",
    "params.ground_truth_units = [36, 83, 199, 243, 267, 283, 464, 1074, 1159]\n",
    "\n",
    "# random seed, for reproducibility\n",
    "params.random_seed = 10191\n",
    "# algorithm to generate hybrid data\n",
    "params.generator_type = \"steinmetz\"\n",
    "# number of singular values to use in the construction of artificial units\n",
    "params.num_singular_values = 8\n",
    "# number of channels to shift the units by\n",
    "params.channel_shift = 20\n",
    "# scale factor for randomly-generated jitter\n",
    "params.time_jitter = 500\n",
    "# minimum amplitude scale factor\n",
    "params.amplitude_scale_min = 0.75\n",
    "# maximum_amplitude_scale_factor\n",
    "params.amplitude_scale_max = 1.5\n",
    "# number of samples to take before an event timestep\n",
    "params.samples_before = 40\n",
    "# number of samples to take after an event timestep\n",
    "params.samples_after = 40\n",
    "# threshold a channel must exceed to be considered part of an event\n",
    "params.event_threshold = -30\n",
    "# point in the raw file at which the data starts\n",
    "params.offset = 0\n",
    "# absolutely do NOT copy this huge file\n",
    "params.copy = False\n",
    "# whether or not to overwrite a target file if it already exists\n",
    "params.overwrite = True\n",
    "# start time of raw data file, in sample units\n",
    "params.start_time = 0\n",
    "# log messages to the screen\n",
    "params.verbose = True\n",
    "\n",
    "SPIKE_LIMIT = 25000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also import our probe module and set the random seed accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import factory.probes.npix3a as probe\n",
    "\n",
    "np.random.seed(params.random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure we're operating on the same files before we do anything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def md5sum(filename):\n",
    "    # hat tip to this guy: https://stackoverflow.com/questions/22058048/hashing-a-file-in-python#22058673\n",
    "    chunk_size = 65536  # read in 64 KiB chunks\n",
    "\n",
    "    result = hashlib.md5()\n",
    "\n",
    "    with open(filename, \"rb\") as fh:\n",
    "        while True:\n",
    "            data = fh.read(chunk_size)\n",
    "            if not data:\n",
    "                break\n",
    "            result.update(data)\n",
    "            \n",
    "    return result.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filenames = [\"Hopkins_20160722_g0_t0.imec.ap_CAR.bin\", \"spike_clusters.npy\", \"spike_templates.npy\",\n",
    "             \"spike_times.npy\", \"templates.npy\"]\n",
    "\n",
    "for filename in filenames:\n",
    "    if not op.isfile(op.join(params.data_directory, filename)):\n",
    "        print(f\"{filename} not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alan\\Documents\\hybridfactory\\notebooks\\demos\\cortex_demo\\Hopkins_20160722_g0_t0.imec.ap_CAR.GT.bin is different! overwriting...done\n"
     ]
    }
   ],
   "source": [
    "assert md5sum(params.raw_source_file) == \"eb93a041e52eba844aed148ac9718998\"\n",
    "\n",
    "# this is somewhat faster than copying a ~80G file\n",
    "if md5sum(params.raw_target_file) != \"eb93a041e52eba844aed148ac9718998\":\n",
    "    print(f\"{params.raw_target_file} is different! overwriting...\", end=\"\")\n",
    "    file_size_bytes = op.getsize(params.raw_source_file)\n",
    "    byte_count = np.dtype(params.data_type).itemsize  # number of bytes in data type\n",
    "    nrows = probe.NCHANS\n",
    "    ncols = file_size_bytes // (nrows * byte_count)\n",
    "\n",
    "    source = np.memmap(params.raw_source_file, dtype=params.data_type, offset=params.offset, mode=\"r\",\n",
    "                       shape=(nrows, ncols), order=\"F\")\n",
    "    target = np.memmap(params.raw_target_file, dtype=params.data_type, offset=params.offset, mode=\"r+\",\n",
    "                       shape=(nrows, ncols), order=\"F\")\n",
    "    \n",
    "    # load up firing times\n",
    "    firings_true = factory.io.gt.load_gt_units(params.data_directory).astype(np.int64)\n",
    "    # reset the target to match up with the source\n",
    "    factory.io.raw.reset_target(source, target, params.samples_before, params.samples_after, firings_true[1, :])\n",
    "    # save\n",
    "    del source, target\n",
    "    print(\"done\")\n",
    "\n",
    "assert md5sum(op.join(params.data_directory, \"spike_clusters.npy\")) == \"d6d49ccbb9e34edc286c161541b681b3\"\n",
    "assert md5sum(op.join(params.data_directory, \"spike_templates.npy\")) == \"218e7748281db5e95babb6b3ebc182c8\"\n",
    "\n",
    "assert md5sum(op.join(params.data_directory, \"spike_times.npy\")) == \"938bc213d15ba9aa2cc9cc84a403c314\"\n",
    "assert md5sum(op.join(params.data_directory, \"templates.npy\")) == \"8328de406b19e0afd35d9aa49c1d6858\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some functions (really just copy them from `generate.py`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _legal_params():\n",
    "    required_params = {\"raw_source_file\": None,\n",
    "                       \"raw_target_file\": None,\n",
    "                       \"data_type\": [np.int16],\n",
    "                       \"sample_rate\": None,\n",
    "                       \"output_type\": [\"kilosort\", \"phy\", \"jrc\"],\n",
    "                       \"data_directory\": None,\n",
    "                       \"probe_type\": [\"npix3a\", \"eMouse\", \"hh2_arseny\"],\n",
    "                       \"ground_truth_units\": None}\n",
    "\n",
    "    optional_params = {\"random_seed\": None,\n",
    "                       \"generator_type\": [\"steinmetz\"],\n",
    "                       \"num_singular_values\": 6,\n",
    "                       \"channel_shift\": None,  # depends on probe\n",
    "                       \"time_jitter\": 500,\n",
    "                       \"amplitude_scale_min\": 0.75,\n",
    "                       \"amplitude_scale_max\": 2.,\n",
    "                       \"samples_before\": 40,\n",
    "                       \"samples_after\": 40,\n",
    "                       \"event_threshold\": -30,\n",
    "                       \"offset\": 0,\n",
    "                       \"copy\": True,\n",
    "                       \"overwrite\": False,\n",
    "                       \"start_time\": 0}\n",
    "\n",
    "    return required_params, optional_params\n",
    "\n",
    "\n",
    "def _write_param(fh, param, param_val):\n",
    "    if param == \"data_type\":\n",
    "        if param_val == np.int16:  # no other data types supported yet\n",
    "            param_val = \"np.int16\"\n",
    "    elif isinstance(param_val, str):  # enclose string in quotes\n",
    "        param_val = f'r\"{param_val}\"'\n",
    "    elif isinstance(param_val, np.ndarray):  # numpy doesn't do roundtripping\n",
    "        param_val = param_val.tolist()\n",
    "    elif param_val is None:\n",
    "        return\n",
    "\n",
    "    print(f\"{param} = {param_val}\", file=fh)\n",
    "\n",
    "\n",
    "def _write_config(filename, params):\n",
    "    required_params, optional_params = _legal_params()\n",
    "\n",
    "    with open(filename, \"w\") as fh:\n",
    "        print(\"import numpy as np\\n\", file=fh)\n",
    "\n",
    "        print(\"# required parameters\\n\", file=fh)\n",
    "        for param in required_params:\n",
    "            _write_param(fh, param, params.__dict__[param])\n",
    "\n",
    "        print(\"\\n# optional parameters\\n\", file=fh)\n",
    "        for param in optional_params:\n",
    "            _write_param(fh, param, params.__dict__[param])\n",
    "\n",
    "        print(f\"# automatically generated on {datetime.datetime.now()}\", file=fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_source_target(params, probe):\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params : module\n",
    "        Session parameters.\n",
    "    probe : module\n",
    "        Probe parameters.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    source : numpy.memmap\n",
    "        Memory map of source data file.\n",
    "    target : numpy.memmap\n",
    "        Memory map of target data file.\n",
    "    \"\"\"\n",
    "\n",
    "    raw_source_files = glob.glob(params.raw_source_file)\n",
    "\n",
    "    if len(raw_source_files) > 1:\n",
    "        assert len(raw_source_files) == len(params.start_time)\n",
    "        \n",
    "        raw_target_files = raw_source_files.copy()\n",
    "        for k, rtf in enumerate(raw_target_files):\n",
    "            # just save hybrid data files in directory containing params file\n",
    "            dirname = op.dirname(rtf)\n",
    "            rtf = rtf.replace(dirname, op.dirname(params.data_directory))\n",
    "\n",
    "            try:\n",
    "                last_dot = -(rtf[::-1].index('.') + 1)\n",
    "                rtf = rtf[:last_dot] + \".GT\" + rtf[last_dot:]  # add \".GT\" before extension\n",
    "            except ValueError:  # no '.' found in rtf\n",
    "                rtf += \".GT\"  # add \".GT\" at the end\n",
    "            finally:\n",
    "                raw_target_files[k] = rtf\n",
    "        start_times = params.start_time\n",
    "    else:\n",
    "        raw_target_files = [params.raw_target_file]\n",
    "        start_times = [params.start_time]\n",
    "\n",
    "    for k, raw_source_file in enumerate(raw_source_files):\n",
    "        start_time = start_times[k]\n",
    "        raw_target_file = raw_target_files[k]\n",
    "\n",
    "        if op.isfile(raw_target_file) and not params.overwrite:\n",
    "            if _user_dialog(f\"Target file {raw_target_file} exists! Overwrite?\") == \"y\":\n",
    "                params.overwrite = True\n",
    "            else:\n",
    "                _err_exit(\"aborting\", 0)\n",
    "\n",
    "        if params.copy:\n",
    "            log(f\"Copying {raw_source_file} to {raw_target_file}\", params.verbose, in_progress=True)\n",
    "            shutil.copy2(raw_source_file, raw_target_file)\n",
    "            log(\"done\", params.verbose)\n",
    "\n",
    "        file_size_bytes = op.getsize(raw_source_file)\n",
    "        byte_count = np.dtype(params.data_type).itemsize  # number of bytes in data type\n",
    "        nrows = probe.NCHANS\n",
    "        ncols = file_size_bytes // (nrows * byte_count)\n",
    "\n",
    "        params.num_samples = ncols\n",
    "\n",
    "        source = np.memmap(raw_source_file, dtype=params.data_type, offset=params.offset, mode=\"r\",\n",
    "                           shape=(nrows, ncols), order=\"F\")\n",
    "        target = np.memmap(raw_target_file, dtype=params.data_type, offset=params.offset, mode=\"r+\",\n",
    "                           shape=(nrows, ncols), order=\"F\")\n",
    "\n",
    "        yield source, target, start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit_channels_union(unit_mask, params, probe):\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    unit_mask : numpy.ndarray\n",
    "        Boolean array of events to take for this unit.\n",
    "    params : module\n",
    "        Session parameters.\n",
    "    probe : module\n",
    "        Probe parameters.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    channels : numpy.ndarray\n",
    "        Channels on which unit events occur.\n",
    "    \"\"\"\n",
    "\n",
    "    # select all channels on which events occur for this unit...\n",
    "    event_channel_indices = factory.io.jrc.load_event_channel_indices(params.data_directory)\n",
    "    channel_neighbor_indices = factory.io.jrc.load_channel_neighbor_indices(params.data_directory)\n",
    "    event_channels = probe.channel_map[probe.connected][event_channel_indices]\n",
    "\n",
    "    # ...find neighbors for all channels...\n",
    "    channel_neighbors = {}\n",
    "    for channel_neighborhood in probe.channel_map[probe.connected][channel_neighbor_indices].T:\n",
    "        channel_neighbors[channel_neighborhood[0]] = set(channel_neighborhood)\n",
    "\n",
    "    # ...and isolate the channels which are neighbors of distinct centers for this unit\n",
    "    unit_channel_centers = np.unique(event_channels[unit_mask])\n",
    "    unit_channels = np.array(list(set.union(*[channel_neighbors[c] for c in unit_channel_centers])))\n",
    "\n",
    "    return unit_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_events(events, params, probe):\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    events : numpy.ndarray\n",
    "        Tensor, num_channels x num_samples x num_events.\n",
    "    params : module\n",
    "        Session parameters.\n",
    "    probe : module\n",
    "        Probe parameters.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    scaled_events : numpy.ndarray\n",
    "        Tensor, num_channels x num_samples x num_events, scaled.\n",
    "    \"\"\"\n",
    "\n",
    "    abs_events = np.abs(events)\n",
    "\n",
    "    centers = abs_events.max(axis=0).argmax(axis=0)\n",
    "\n",
    "    scale_factors = np.random.uniform(params.amplitude_scale_min, params.amplitude_scale_max, size=abs_events.shape[2])\n",
    "    scale_rows = [np.hstack((np.linspace(0, scale_factors[i], centers[i]),\n",
    "                  np.linspace(scale_factors[i], 0, events.shape[1]-centers[i]+1)[1:]))[np.newaxis, :] for i in range(events.shape[2])]\n",
    "\n",
    "    return np.stack(scale_rows, axis=2) * events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading event times and cluster IDs ... done\n",
      "Generating ground truth for unit 36 ... done\n",
      "Shifting channels ... done\n",
      "Jittering events ... done\n",
      "Writing events to file ... done\n",
      "Generating ground truth for unit 83 ... done\n",
      "Shifting channels ... done\n",
      "Jittering events ... done\n",
      "Writing events to file ... done\n",
      "Generating ground truth for unit 199 ... no channels found for unit\n",
      "Generating ground truth for unit 243 ... done\n",
      "Shifting channels ... done\n",
      "Jittering events ... done\n",
      "Writing events to file ... done\n",
      "Generating ground truth for unit 267 ... done\n",
      "Shifting channels ... channel shift of 20 places events on unconnected channels\n",
      "Generating ground truth for unit 283 ... no channels found for unit\n",
      "Generating ground truth for unit 464 ... done\n",
      "Shifting channels ... done\n",
      "Jittering events ... done\n",
      "Writing events to file ... done\n",
      "Generating ground truth for unit 1074 ... done\n",
      "Shifting channels ... done\n",
      "Jittering events ... done\n",
      "Writing events to file ... done\n",
      "Generating ground truth for unit 1159 ... done\n",
      "Shifting channels ... done\n",
      "Jittering events ... done\n",
      "Writing events to file ... done\n",
      "Firing times and labels saved to F:/CortexLab/singlePhase3/data\\firings_true.npy.\n",
      "Parameter file to recreate this run saved at F:/CortexLab/singlePhase3/data\\params-demo.py.\n"
     ]
    }
   ],
   "source": [
    "log(\"Loading event times and cluster IDs\", params.verbose, in_progress=True)\n",
    "\n",
    "import factory.io.phy as io\n",
    "event_times = io.load_event_times(params.data_directory)\n",
    "event_clusters = io.load_event_clusters(params.data_directory)\n",
    "log(\"done\", params.verbose)\n",
    "\n",
    "gt_channels = []\n",
    "gt_times = []\n",
    "gt_labels = []\n",
    "\n",
    "for source, target, start_time in copy_source_target(params, probe):\n",
    "    time_mask = (event_times - params.samples_before >= start_time) & (event_times - start_time +\n",
    "                                                                       params.samples_after < target.shape[1])\n",
    "\n",
    "    for unit_id in params.ground_truth_units:\n",
    "        unit_mask = (event_clusters == unit_id) & time_mask\n",
    "        num_events = np.where(unit_mask)[0].size\n",
    "\n",
    "        if num_events > SPIKE_LIMIT:  # if more events than limit, select some to ignore\n",
    "            falsify = np.random.choice(np.where(unit_mask)[0], size=num_events-SPIKE_LIMIT, replace=False)\n",
    "            unit_mask[falsify] = False\n",
    "        elif num_events == 0:\n",
    "            log(f\"No events found for unit {unit_id}\", params.verbose)\n",
    "            continue\n",
    "\n",
    "        # generate artificial events for this unit\n",
    "        log(f\"Generating ground truth for unit {unit_id}\", params.verbose, in_progress=True)\n",
    "\n",
    "        unit_times = event_times[unit_mask] - start_time\n",
    "        unit_windows = factory.io.raw.unit_windows(source, unit_times, params.samples_before, params.samples_after)\n",
    "        unit_windows[probe.channel_map[~probe.connected], :, :] = 0  # zero out the unconnected channels\n",
    "\n",
    "        if params.output_type == \"jrc\":\n",
    "            unit_channels = unit_channels_union(unit_mask, params, probe)\n",
    "        else:\n",
    "            unit_channels = factory.generate.generators.threshold_events(unit_windows, params.event_threshold)\n",
    "\n",
    "        if unit_channels is None:\n",
    "            log(\"no channels found for unit\", params.verbose)\n",
    "            continue\n",
    "\n",
    "        # now create subarray for just appropriate channels\n",
    "        events = unit_windows[unit_channels, :, :]  # num_channels x num_samples x num_events\n",
    "\n",
    "        # actually generate the data\n",
    "        if params.generator_type == \"steinmetz\":\n",
    "            if num_events < params.num_singular_values:\n",
    "                log(\"not enough events to generate!\", params.verbose)\n",
    "                continue\n",
    "            art_events = factory.generate.generators.steinmetz(events, params.num_singular_values)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"generator '{params.generator_type}' does not exist!\")\n",
    "\n",
    "        art_events = scale_events(art_events, params, probe)\n",
    "\n",
    "        log(\"done\", params.verbose)\n",
    "\n",
    "        # shift channels\n",
    "        log(\"Shifting channels\", params.verbose, in_progress=True)\n",
    "        shifted_channels = factory.generate.shift.shift_channels(unit_channels, params, probe)\n",
    "\n",
    "        if shifted_channels is None:\n",
    "            continue  # cause is logged in `shift_channels`\n",
    "\n",
    "        log(\"done\", params.verbose)\n",
    "\n",
    "        # jitter events\n",
    "        log(\"Jittering events\", params.verbose, in_progress=True)\n",
    "        jittered_times = factory.generate.shift.jitter_events(unit_times, params)\n",
    "        log(\"done\", params.verbose)\n",
    "\n",
    "        if jittered_times is None:\n",
    "            continue\n",
    "\n",
    "        # write to file\n",
    "        log(\"Writing events to file\", params.verbose, in_progress=True)\n",
    "        for i, jittered_center in enumerate(jittered_times):\n",
    "            jittered_samples = np.arange(jittered_center - params.samples_before,\n",
    "                                         jittered_center + params.samples_after + 1, dtype=jittered_center.dtype)\n",
    "\n",
    "            shifted_window = factory.io.raw.read_roi(target, shifted_channels, jittered_samples)\n",
    "            perturbed_data = shifted_window + art_events[:, :, i]\n",
    "\n",
    "            factory.io.raw.write_roi(target, shifted_channels, jittered_samples, perturbed_data)\n",
    "\n",
    "        log(\"done\", params.verbose)\n",
    "\n",
    "        cc_indices = np.abs(art_events).max(axis=1).argmax(axis=0)\n",
    "        center_channels = shifted_channels[cc_indices] + 1\n",
    "\n",
    "        gt_channels.append(center_channels)\n",
    "        gt_times.append(jittered_times + start_time)\n",
    "        gt_labels.append(unit_id)\n",
    "\n",
    "    # finished writing, flush to file\n",
    "    del source, target\n",
    "\n",
    "# save everything for later\n",
    "dirname = params.data_directory\n",
    "\n",
    "# save ground-truth units for validation\n",
    "filename = factory.io.gt.save_gt_units(dirname, gt_channels, gt_times, gt_labels)\n",
    "log(f\"Firing times and labels saved to {filename}.\", params.verbose)\n",
    "\n",
    "# save parameter file for later reuse\n",
    "filename = op.join(dirname, f\"params-demo.py\")\n",
    "_write_config(filename, params)\n",
    "log(f\"Parameter file to recreate this run saved at {filename}.\", params.verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert md5sum(params.raw_target_file) == \"f0a179225e0b173e55efa0265d5dea2c\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! You now have a reproducible hybrid ground-truth file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
